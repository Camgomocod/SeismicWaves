{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b3a235",
   "metadata": {},
   "source": [
    "# Wavelet Transform + CNN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde987ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Paths\n",
    "augmented_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented'\n",
    "features_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features'\n",
    "\n",
    "# Create features directory if it doesn't exist\n",
    "if not os.path.exists(features_path):\n",
    "    os.makedirs(features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3f06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wavelet features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4989/4989 [03:34<00:00, 23.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (4989, 60)\n",
      "Number of samples: 4989\n",
      "Features extracted per coefficient level: 12\n",
      "Total features for 4 levels: 60\n"
     ]
    }
   ],
   "source": [
    "def extract_wavelet_features(signal, wavelet='db4', level=4):\n",
    "    \"\"\"Extract statistical features from wavelet decomposition of a signal.\n",
    "    Args:\n",
    "        signal: Input signal array\n",
    "        wavelet: Wavelet type to use\n",
    "        level: Decomposition level\n",
    "    Returns:\n",
    "        array: Feature vector containing statistical measures\"\"\"\n",
    "    # Perform wavelet decomposition\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    \n",
    "    # Initialize feature list\n",
    "    features = []\n",
    "    \n",
    "    # Extract features from each coefficient level\n",
    "    for coef in coeffs:\n",
    "        # Statistical features\n",
    "        features.extend([\n",
    "            np.mean(coef),           # Mean\n",
    "            np.std(coef),            # Standard deviation\n",
    "            stats.skew(coef),        # Skewness\n",
    "            stats.kurtosis(coef),    # Kurtosis\n",
    "            np.percentile(coef, 75), # 75th percentile\n",
    "            np.percentile(coef, 25), # 25th percentile\n",
    "            np.max(coef),            # Maximum\n",
    "            np.min(coef),            # Minimum\n",
    "            np.sum(np.abs(coef)),    # L1 norm\n",
    "            np.sqrt(np.sum(coef**2)),# L2 norm\n",
    "            stats.entropy(np.abs(coef)), # Signal entropy\n",
    "            np.median(np.abs(coef))  # Median absolute deviation\n",
    "        ])\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "def process_seismic_files(data_path, arrival_times_csv):\n",
    "    \"\"\"Process all seismic files and extract wavelet features.\n",
    "    Args:\n",
    "        data_path: Path to directory containing MSEED files\n",
    "        arrival_times_csv: Path to CSV with arrival times\n",
    "    Returns:\n",
    "        tuple: (features array, arrival times array, file names)\"\"\"\n",
    "    # Read arrival times\n",
    "    arrivals_df = pd.read_csv(arrival_times_csv)\n",
    "    \n",
    "    features_list = []\n",
    "    arrival_times = []\n",
    "    file_names = []\n",
    "    \n",
    "    print('Extracting wavelet features...')\n",
    "    for _, row in tqdm(arrivals_df.iterrows(), total=len(arrivals_df)):\n",
    "        file_path = os.path.join(data_path, row['augmented_file'])\n",
    "        \n",
    "        try:\n",
    "            # Read seismic signal\n",
    "            st = read(file_path)\n",
    "            signal = st[0].data\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_wavelet_features(signal)\n",
    "            features_list.append(features)\n",
    "            arrival_times.append(row['arrival_time'])\n",
    "            file_names.append(row['augmented_file'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file_path}: {str(e)}')\n",
    "            continue\n",
    "    \n",
    "    return np.array(features_list), np.array(arrival_times), file_names\n",
    "\n",
    "# Process all files\n",
    "augmented_path = os.path.join(augmented_data_path, 'augmented')\n",
    "arrival_times_csv = os.path.join(augmented_data_path, 'arrival_times.csv')\n",
    "\n",
    "X, y, files = process_seismic_files(augmented_path, arrival_times_csv)\n",
    "\n",
    "# Save features and metadata\n",
    "np.save(os.path.join(features_path, 'wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'feature_files.csv'), index=False)\n",
    "\n",
    "print(f'Extracted features shape: {X.shape}')\n",
    "print(f'Number of samples: {len(files)}')\n",
    "\n",
    "# Update the feature extraction info\n",
    "print('Features extracted per coefficient level:', 12)\n",
    "print('Total features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331c7440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for file 01010056.mseed:\n",
      "Feature vector length: 60\n",
      "\n",
      "First 10 features:\n",
      "[-2.26887150e-03  1.36387755e-01  2.11445763e+00  9.02215459e+01\n",
      "  1.74969649e-02 -1.97850397e-02  1.76067501e+00 -1.31033293e+00\n",
      "  1.88985862e+01  2.85151844e+00]\n",
      "\n",
      "Arrival time: 30.60s\n"
     ]
    }
   ],
   "source": [
    "# Show example of features for one file\n",
    "example_idx = 0\n",
    "print(f'Features for file {files[example_idx]}:')\n",
    "print('Feature vector length:', len(X[example_idx]))\n",
    "print('\\nFirst 10 features:')\n",
    "print(X[example_idx][:10])\n",
    "print(f'\\nArrival time: {y[example_idx]:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2936c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X: (4989, 60)\n",
      "Forma de y: (4989,)\n",
      "\n",
      "Primeros 5 elementos de X:\n",
      "[[-2.26887150e-03  1.36387755e-01  2.11445763e+00  9.02215459e+01\n",
      "   1.74969649e-02 -1.97850397e-02  1.76067501e+00 -1.31033293e+00\n",
      "   1.88985862e+01  2.85151844e+00  4.90902773e+00  1.87466164e-02\n",
      "  -4.68282065e-03  4.36041154e-01 -1.00966551e+00  3.50142234e+01\n",
      "   7.43629529e-02 -8.48785630e-02  3.57025540e+00 -3.56212458e+00\n",
      "   7.44136070e+01  9.11576754e+00  5.07205141e+00  8.28264720e-02\n",
      "   5.64923997e-02  1.62441522e+00  4.88074603e+00  1.10670185e+02\n",
      "   2.23716437e-01 -2.32970680e-01  2.68300654e+01 -1.51858868e+01\n",
      "   4.54496807e+02  4.78871929e+01  5.62304109e+00  2.28371860e-01\n",
      "   3.33488387e-05  1.16491400e+00 -5.91348512e-01  7.28446100e+01\n",
      "   1.35390393e-01 -1.25288355e-01  1.26437588e+01 -1.69246391e+01\n",
      "   6.01567335e+02  4.84525808e+01  6.10403926e+00  1.28995578e-01\n",
      "  -1.18978216e-05  1.56823783e-01 -9.33989454e-01  7.02097351e+01\n",
      "   2.04853454e-02 -1.97404723e-02  1.95495840e+00 -2.39828782e+00\n",
      "   1.75673647e+02  9.21664998e+00  6.91394816e+00  2.01186373e-02]\n",
      " [-2.78073911e-03  1.31386908e-01  1.97194159e+00  8.47180714e+01\n",
      "   1.73914060e-02 -1.98043740e-02  1.66836012e+00 -1.24163016e+00\n",
      "   1.86626542e+01  2.74719860e+00  4.92488746e+00  1.89794194e-02\n",
      "  -4.04438145e-03  4.15182662e-01 -9.96387607e-01  3.43010655e+01\n",
      "   7.87078891e-02 -8.55785640e-02  3.38306144e+00 -3.37535693e+00\n",
      "   7.23412199e+01  8.67961641e+00  5.09868660e+00  8.28520392e-02\n",
      "   5.46658550e-02  1.54269890e+00  4.84560726e+00  1.09643018e+02\n",
      "   2.30213017e-01 -2.22895235e-01  2.54233239e+01 -1.43896675e+01\n",
      "   4.40372299e+02  4.54792740e+01  5.65307839e+00  2.27238778e-01\n",
      "   4.41489196e-05  1.10511105e+00 -5.89324704e-01  7.24956452e+01\n",
      "   1.28934983e-01 -1.19759040e-01  1.19808272e+01 -1.60372543e+01\n",
      "   5.79545884e+02  4.59651809e+01  6.13390489e+00  1.25805948e-01\n",
      "  -1.23356931e-05  1.48794737e-01 -9.30173622e-01  6.98308067e+01\n",
      "   1.98003277e-02 -1.96806573e-02  1.85245693e+00 -2.27254191e+00\n",
      "   1.69187629e+02  8.74477698e+00  6.94216426e+00  1.97477541e-02]\n",
      " [ 7.01690447e-03  1.43728394e-01  2.12691936e+00  7.49080174e+01\n",
      "   2.41702330e-02 -2.07696069e-02  1.76624451e+00 -1.33278114e+00\n",
      "   2.17542970e+01  3.00815517e+00  5.00962437e+00  2.20897215e-02\n",
      "  -1.64237927e-03  4.36704361e-01 -1.01777622e+00  3.48523135e+01\n",
      "   8.13999749e-02 -7.82508967e-02  3.56858606e+00 -3.55995188e+00\n",
      "   7.49955563e+01  9.12917050e+00  5.08452950e+00  8.03226011e-02\n",
      "   6.38297538e-02  1.62470216e+00  4.87316402e+00  1.10620749e+02\n",
      "   2.28907246e-01 -2.17910985e-01  2.68384265e+01 -1.51715690e+01\n",
      "   4.53853184e+02  4.79036408e+01  5.61654119e+00  2.24692355e-01\n",
      "  -4.67675871e-04  1.16439617e+00 -5.92529615e-01  7.28881320e+01\n",
      "   1.30003263e-01 -1.24509670e-01  1.26476890e+01 -1.69280137e+01\n",
      "   6.02112469e+02  4.84310468e+01  6.10828156e+00  1.28185975e-01\n",
      "  -2.12634018e-04  1.57078457e-01 -9.74028372e-01  6.88163556e+01\n",
      "   2.42115903e-02 -2.49278539e-02  1.94735878e+00 -2.41084441e+00\n",
      "   1.88142164e+02  9.23162578e+00  7.04345344e+00  2.45201043e-02]\n",
      " [ 4.95318652e-03  1.08089806e-01  6.76946493e-01  1.78925549e+01\n",
      "   2.59214767e-02 -2.53102785e-02  6.72727573e-01 -7.78559480e-01\n",
      "   2.50276758e+01  2.34329341e+00  5.37874545e+00  2.55920848e-02\n",
      "  -2.13726744e-03  3.87532334e-01  2.97102655e-01  1.66994858e+01\n",
      "   1.34302319e-01 -1.26973936e-01  3.02161880e+00 -2.48973620e+00\n",
      "   1.02245272e+02  8.39268592e+00  5.53249019e+00  1.30336628e-01\n",
      "  -1.84671474e-02  1.20351184e+00 -7.35053160e-01  2.17897817e+01\n",
      "   3.58279117e-01 -3.76788132e-01  8.98613364e+00 -1.15170212e+01\n",
      "   6.11392336e+02  3.67262282e+01  6.19376512e+00  3.73615295e-01\n",
      "  -4.59995528e-04  7.16549085e-01  5.12237423e-02  2.46330961e+01\n",
      "   2.28383962e-01 -2.32447131e-01  7.54858748e+00 -8.08766148e+00\n",
      "   7.29819672e+02  3.08615683e+01  6.90263162e+00  2.31413748e-01\n",
      "  -1.94334647e-05  1.18638447e-01  3.35999434e-01  3.09023320e+01\n",
      "   3.90900068e-02 -3.81517114e-02  1.48575353e+00 -1.25993729e+00\n",
      "   2.37555885e+02  7.22039484e+00  7.59830720e+00  3.85905082e-02]\n",
      " [ 5.58398012e-03  1.24832997e-01  5.60219118e-01  1.90629747e+01\n",
      "   2.85865890e-02 -2.75337414e-02  7.97672649e-01 -9.23160619e-01\n",
      "   2.81323541e+01  2.70613761e+00  5.34763948e+00  2.82217202e-02\n",
      "  -2.93711762e-03  4.50919196e-01  3.17860601e-01  1.81470702e+01\n",
      "   1.46363010e-01 -1.40027766e-01  3.58282129e+00 -2.95215262e+00\n",
      "   1.15346824e+02  9.76549716e+00  5.49970138e+00  1.45330268e-01\n",
      "  -2.08509938e-02  1.39025909e+00 -7.82802396e-01  2.41838283e+01\n",
      "   4.06470213e-01 -4.23188230e-01  1.06551200e+01 -1.36560670e+01\n",
      "   6.85714238e+02  4.24247629e+01  6.16645337e+00  4.10516821e-01\n",
      "  -5.17589674e-04  8.24856575e-01  5.49725942e-02  2.75871766e+01\n",
      "   2.50967439e-01 -2.55328692e-01  8.95057972e+00 -9.58977544e+00\n",
      "   8.15893504e+02  3.55263412e+01  6.87762528e+00  2.53457455e-01\n",
      "  -2.02853577e-05  1.37386423e-01  3.59516500e-01  3.40930719e+01\n",
      "   4.31861666e-02 -4.21525851e-02  1.76170118e+00 -1.49394429e+00\n",
      "   2.66069039e+02  8.36140596e+00  7.56995287e+00  4.26432622e-02]]\n",
      "\n",
      "Primeros 5 elementos de y:\n",
      "[30.5999999 30.5999999 20.9999999 30.0999999 30.0999999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cargar los archivos\n",
    "features_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features'\n",
    "X = np.load(os.path.join(features_path, 'wavelet_features.npy'))\n",
    "y = np.load(os.path.join(features_path, 'arrival_times.npy'))\n",
    "\n",
    "# Verificar las dimensiones\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "\n",
    "# Ver los primeros elementos\n",
    "print(\"\\nPrimeros 5 elementos de X:\")\n",
    "print(X[:5])\n",
    "print(\"\\nPrimeros 5 elementos de y:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013272bc",
   "metadata": {},
   "source": [
    "## Preprocesamiento de señales\n",
    "\n",
    "Para entrenar una CNN, todas las señales de entrada deben tener la misma longitud. Hay tres casos posibles:\n",
    "\n",
    "1. La señal tiene exactamente 3000 muestras → se usa tal cual\n",
    "2. La señal tiene más de 3000 muestras → se corta al tamaño deseado\n",
    "3. La señal tiene menos de 3000 muestras → se rellena con ceros hasta alcanzar 3000\n",
    "\n",
    "Esto asegura que todas las señales tengan dimensiones consistentes para el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58ee81",
   "metadata": {},
   "source": [
    "## Carga de datos preprocesados\n",
    "\n",
    "Las señales sísmicas ya han sido preprocesadas con:\n",
    "1. Normalización Z-score\n",
    "2. Filtro pasabanda (bandpass filter)\n",
    "\n",
    "Por lo tanto, usaremos las señales directamente sin procesamiento adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766569fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar datos de wavelets y arrival times\n",
    "X_wavelets = np.load(os.path.join(features_path, 'wavelet_features.npy'))\n",
    "y = np.load(os.path.join(features_path, 'arrival_times.npy'))\n",
    "\n",
    "# Cargar señales preprocesadas\n",
    "raw_signals = []\n",
    "files_df = pd.read_csv(os.path.join(features_path, 'feature_files.csv'))\n",
    "\n",
    "print('Cargando señales preprocesadas...')\n",
    "for file in tqdm(files_df['file']):\n",
    "    file_path = os.path.join(augmented_data_path, 'augmented', file)\n",
    "    st = read(file_path)\n",
    "    signal = st[0].data\n",
    "    raw_signals.append(signal)\n",
    "\n",
    "X_raw = np.array(raw_signals)\n",
    "# Ajustar dimensiones para CNN\n",
    "X_raw = X_raw.reshape(X_raw.shape[0], -1, 1)\n",
    "\n",
    "print(f'Forma de las señales crudas: {X_raw.shape}')\n",
    "print(f'Forma de características wavelets: {X_wavelets.shape}')\n",
    "\n",
    "# Dividir los datos\n",
    "X_raw_train, X_raw_test, X_wav_train, X_wav_test, y_train, y_test = train_test_split(\n",
    "    X_raw, X_wavelets, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo híbrido\n",
    "# Entrada para señales crudas\n",
    "raw_input = tf.keras.layers.Input(shape=(3000, 1))\n",
    "x1 = tf.keras.layers.Conv1D(32, 5, activation='relu')(raw_input)\n",
    "x1 = tf.keras.layers.MaxPooling1D(2)(x1)\n",
    "x1 = tf.keras.layers.Conv1D(64, 5, activation='relu')(x1)\n",
    "x1 = tf.keras.layers.MaxPooling1D(2)(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "# Entrada para características wavelets\n",
    "wavelet_input = tf.keras.layers.Input(shape=(X_wavelets.shape[1],))\n",
    "x2 = tf.keras.layers.Dense(64, activation='relu')(wavelet_input)\n",
    "x2 = tf.keras.layers.Dropout(0.3)(x2)\n",
    "\n",
    "# Combinar ambas entradas\n",
    "combined = tf.keras.layers.concatenate([x1, x2])\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[raw_input, wavelet_input], outputs=output)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    [X_raw_train, X_wav_train],\n",
    "    y_train,\n",
    "    validation_data=([X_raw_test, X_wav_test], y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc596f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entrenamiento\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_mae = model.evaluate([X_raw_test, X_wav_test], y_test)\n",
    "print(f'\\nTest Loss: {test_loss:.4f}')\n",
    "print(f'Test MAE: {test_mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavePredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
