{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7a0d40",
   "metadata": {},
   "source": [
    "# Wavelet Transform + CNN Feature Extraction\n",
    "\n",
    "This notebook implements wavelet-based feature extraction for seismic signals combined with CNN processing. The main components are:\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Wavelet Decomposition**\n",
    "   - Uses db4 wavelet with 4 levels\n",
    "   - Extracts statistical features from coefficients\n",
    "   - Handles multiple signal scales\n",
    "\n",
    "2. **Feature Extraction**\n",
    "   - Statistical measures (mean, std, skewness, etc.)\n",
    "   - Signal characteristics (entropy, norms)\n",
    "   - Wavelet coefficient analysis\n",
    "\n",
    "3. **Data Processing**\n",
    "   - Training data augmentation\n",
    "   - Validation split handling\n",
    "   - Test set preparation\n",
    "\n",
    "## Feature Organization\n",
    "\n",
    "Features are extracted for each wavelet coefficient level including:\n",
    "- Mean and standard deviation\n",
    "- Skewness and kurtosis\n",
    "- Percentile measures\n",
    "- Signal entropy\n",
    "- L1 and L2 norms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050c610",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import required libraries and configure paths:\n",
    "\n",
    "- **Signal Processing**: numpy, scipy, pywt\n",
    "- **Data Handling**: pandas, obspy\n",
    "- **Visualization**: matplotlib\n",
    "- **File Management**: os, tqdm\n",
    "\n",
    "Define paths for:\n",
    "- Augmented data storage\n",
    "- Feature extraction outputs\n",
    "- Model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6197c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import read\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "\n",
    "# Paths\n",
    "augmented_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented'\n",
    "features_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features'\n",
    "\n",
    "# Create features directory if it doesn't exist\n",
    "if not os.path.exists(features_path):\n",
    "    os.makedirs(features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e35c8",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "Core functions for processing seismic signals:\n",
    "\n",
    "1. **extract_wavelet_features**:\n",
    "   - Performs wavelet decomposition\n",
    "   - Calculates statistical measures\n",
    "   - Generates feature vectors\n",
    "\n",
    "2. **process_seismic_files**:\n",
    "   - Batch processes MSEED files\n",
    "   - Extracts features for each signal\n",
    "   - Maintains arrival time mapping\n",
    "\n",
    "3. **match_arrival_times**:\n",
    "   - Aligns signals with P-wave arrivals\n",
    "   - Converts absolute to relative times\n",
    "   - Validates data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": 2,
   "id": "9f61ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(signal, wavelet='db4', level=4):\n",
    "    \"\"\"Extract statistical features from wavelet decomposition of a signal.\n",
    "    Args:\n",
    "        signal: Input signal array\n",
    "        wavelet: Wavelet type to use\n",
    "        level: Decomposition level\n",
    "    Returns:\n",
    "        array: Feature vector containing statistical measures\"\"\"\n",
    "    # Perform wavelet decomposition\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    \n",
    "    # Initialize feature list\n",
    "    features = []\n",
    "    \n",
    "    # Extract features from each coefficient level\n",
    "    for coef in coeffs:\n",
    "        # Statistical features\n",
    "        features.extend([\n",
    "            np.mean(coef),           # Mean\n",
    "            np.std(coef),            # Standard deviation\n",
    "            stats.skew(coef),        # Skewness\n",
    "            stats.kurtosis(coef),    # Kurtosis\n",
    "            np.percentile(coef, 75), # 75th percentile\n",
    "            np.percentile(coef, 25), # 25th percentile\n",
    "            np.max(coef),            # Maximum\n",
    "            np.min(coef),            # Minimum\n",
    "            np.sum(np.abs(coef)),    # L1 norm\n",
    "            np.sqrt(np.sum(coef**2)),# L2 norm\n",
    "            stats.entropy(np.abs(coef)), # Signal entropy\n",
    "            np.median(np.abs(coef))  # Median absolute deviation\n",
    "        ])\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "def process_seismic_files(data_path, arrival_times_csv):\n",
    "    \"\"\"Process all seismic files and extract wavelet features.\n",
    "    Args:\n",
    "        data_path: Path to directory containing MSEED files\n",
    "        arrival_times_csv: Path to CSV with arrival times\n",
    "    Returns:\n",
    "        tuple: (features array, arrival times array, file names)\"\"\"\n",
    "    # Read arrival times\n",
    "    arrivals_df = pd.read_csv(arrival_times_csv)\n",
    "    \n",
    "    features_list = []\n",
    "    arrival_times = []\n",
    "    file_names = []\n",
    "    \n",
    "    print('Extracting wavelet features...')\n",
    "    for _, row in tqdm(arrivals_df.iterrows(), total=len(arrivals_df)):\n",
    "        file_path = os.path.join(data_path, row['file'])\n",
    "        \n",
    "        try:\n",
    "            # Read seismic signal\n",
    "            st = read(file_path)\n",
    "            signal = st[0].data\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_wavelet_features(signal)\n",
    "            features_list.append(features)\n",
    "            arrival_times.append(row['arrival_time'])\n",
    "            file_names.append(row['file'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error processing {file_path}: {str(e)}')\n",
    "            continue\n",
    "    \n",
    "    return np.array(features_list), np.array(arrival_times), file_names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f06a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wavelet features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4989/4989 [03:34<00:00, 23.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (4989, 60)\n",
      "Number of samples: 4989\n",
      "Features extracted per coefficient level: 12\n",
      "Total features for 4 levels: 60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process all files\n",
    "augmented_path = os.path.join(augmented_data_path, 'augmented')\n",
    "arrival_times_csv = os.path.join(augmented_data_path, 'arrival_times.csv')\n",
    "\n",
    "X, y, files = process_seismic_files(augmented_path, arrival_times_csv)\n",
    "\n",
    "# Save features and metadata\n",
    "np.save(os.path.join(features_path, 'wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'feature_files.csv'), index=False)\n",
    "\n",
    "print(f'Extracted features shape: {X.shape}')\n",
    "print(f'Number of samples: {len(files)}')\n",
    "\n",
    "# Update the feature extraction info\n",
    "print('Features extracted per coefficient level:', 12)\n",
    "print('Total features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wavelet features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/317 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317/317 [00:14<00:00, 22.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation features shape: (317, 60)\n",
      "Number of validation samples: 317\n",
      "Validation features extracted per coefficient level: 12\n",
      "Total validation features for 4 levels: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process validation files\n",
    "val_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/val'\n",
    "val_arrival_times_csv = os.path.join(features_path, 'val_arrival_times.csv')\n",
    "\n",
    "X, y, files = process_seismic_files(val_path, val_arrival_times_csv)\n",
    "# Save validation features and metadata\n",
    "np.save(os.path.join(features_path, 'val_wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'val_arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'val_feature_files.csv'), index=False)\n",
    "print(f'Validation features shape: {X.shape}')\n",
    "print(f'Number of validation samples: {len(files)}')\n",
    "print('Validation features extracted per coefficient level:', 12)\n",
    "print('Total validation features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757c78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wavelet features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 496/496 [00:23<00:00, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (496, 60)\n",
      "Number of test samples: 496\n",
      "Test features extracted per coefficient level: 12\n",
      "Total test features for 4 levels: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process test files\n",
    "test_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/testing'\n",
    "test_arrival_times_csv = os.path.join(features_path, 'test_arrival_times.csv')\n",
    "\n",
    "X, y, files = process_seismic_files(test_path, test_arrival_times_csv)\n",
    "# Save test features and metadata\n",
    "np.save(os.path.join(features_path, 'test_wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'test_arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'test_feature_files.csv'), index=False)\n",
    "print(f'Test features shape: {X.shape}')\n",
    "print(f'Number of test samples: {len(files)}')\n",
    "print('Test features extracted per coefficient level:', 12)\n",
    "print('Total test features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fe23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting wavelet features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1663/1663 [01:13<00:00, 22.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (1663, 60)\n",
      "Number of train samples: 1663\n",
      "Train features extracted per coefficient level: 12\n",
      "Total train features for 4 levels: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process training original files not augmented\n",
    "train_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/train'\n",
    "train_arrival_times_csv = os.path.join(features_path, 'train_arrival_times.csv')\n",
    "X, y, files = process_seismic_files(train_path, train_arrival_times_csv)\n",
    "\n",
    "np.save(os.path.join(features_path, 'train_wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'train_arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'train_feature_files.csv'), index=False)\n",
    "print(f'Train features shape: {X.shape}')\n",
    "print(f'Number of train samples: {len(files)}')\n",
    "print('Train features extracted per coefficient level:', 12)\n",
    "print('Total train features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training range augmented files\n",
    "augmented_range_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/augmented_ranges'\n",
    "train_range_arrival_times_csv = os.path.join(augmented_data_path, 'arrival_times_all.csv')\n",
    "X, y, files = process_seismic_files(augmented_range_path, train_range_arrival_times_csv)\n",
    "\n",
    "np.save(os.path.join(features_path, 'train_range_wavelet_features.npy'), X)\n",
    "np.save(os.path.join(features_path, 'train_range_arrival_times.npy'), y)\n",
    "pd.DataFrame({'file': files}).to_csv(\n",
    "    os.path.join(features_path, 'train_range_feature_files.csv'), index=False)\n",
    "print(f'Train features shape: {X.shape}')\n",
    "print(f'Number of train samples: {len(files)}')\n",
    "print('Train features extracted per coefficient level:', 12)\n",
    "print('Total train features for 4 levels:', 12 * (4 + 1))  # 4 detail + 1 approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d45fb",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline\n",
    "\n",
    "Process different data splits:\n",
    "\n",
    "1. **Training Data**\n",
    "   - Original signals\n",
    "   - Augmented versions\n",
    "   - Range-based augmentations\n",
    "\n",
    "2. **Validation Data**\n",
    "   - Held-out validation set\n",
    "   - Feature extraction\n",
    "   - Arrival time matching\n",
    "\n",
    "3. **Test Data**\n",
    "   - Separate test set\n",
    "   - Independent validation\n",
    "   - Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccc650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_arrival_times(directory_path, csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Extrae tiempos de llegada del CSV que coincidan con archivos MSEED en el directorio.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Ruta al directorio con archivos MSEED\n",
    "        csv_path: Ruta al CSV con los tiempos de llegada\n",
    "        output_path: Ruta donde guardar el nuevo CSV\n",
    "    \"\"\"\n",
    "    # Leer el CSV original\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Obtener lista de archivos MSEED en el directorio\n",
    "    mseed_files = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith('.mseed'):\n",
    "            # Extraer el número del archivo sin la extensión\n",
    "            file_number = int(file.replace('.mseed', ''))\n",
    "            mseed_files.append(file_number)\n",
    "    \n",
    "    # Filtrar el DataFrame para mantener solo las filas que coinciden con los archivos\n",
    "    matched_df = df[df['archivo'].isin(mseed_files)].copy()\n",
    "    \n",
    "    # Calcular tiempos relativos\n",
    "    arrival_times = []\n",
    "    filenames = []\n",
    "    \n",
    "    for _, row in tqdm(matched_df.iterrows(), desc='Procesando archivos'):\n",
    "        file_id = row['archivo']\n",
    "        mseed_file = f\"{file_id:08d}.mseed\"\n",
    "        file_path = os.path.join(directory_path, mseed_file)\n",
    "        \n",
    "        try:\n",
    "            # Leer señal y calcular tiempo relativo\n",
    "            st = read(file_path)\n",
    "            absolute_p_time = row['lec_p']\n",
    "            relative_p_time = absolute_p_time - st[0].stats.starttime.timestamp\n",
    "            \n",
    "            arrival_times.append(relative_p_time)\n",
    "            filenames.append(mseed_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {mseed_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Crear nuevo DataFrame con los resultados\n",
    "    result_df = pd.DataFrame({\n",
    "        'file': filenames,\n",
    "        'arrival_time': arrival_times\n",
    "    })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Se guardaron {len(result_df)} coincidencias en {output_path}\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5b3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 496it [00:17, 28.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se guardaron 496 coincidencias en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/test_arrival_times.csv\n",
      "\n",
      "Primeras 5 coincidencias de test:\n",
      "             file  arrival_time\n",
      "0  04031646.mseed          29.6\n",
      "1  04100031.mseed          30.6\n",
      "2  04110253.mseed          30.5\n",
      "3  04150010.mseed          30.4\n",
      "4  04152328.mseed          30.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test arrival times \n",
    "test_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/testing'\n",
    "output_test_path = os.path.join(features_path, 'test_arrival_times.csv')\n",
    "matched_test_times = match_arrival_times(test_path, csv_path, output_test_path)\n",
    "# Mostrar las primeras filas del resultado\n",
    "print(\"\\nPrimeras 5 coincidencias de test:\")\n",
    "print(matched_test_times.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002b0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 1663it [01:21, 20.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se guardaron 1663 coincidencias en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/train_arrival_times.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/train'\n",
    "output_training_path = os.path.join(features_path, 'train_arrival_times.csv')\n",
    "matched_train_times = match_arrival_times(train_path, csv_path, output_training_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331c7440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for file 01010056.mseed:\n",
      "Feature vector length: 60\n",
      "\n",
      "First 10 features:\n",
      "[-2.26887150e-03  1.36387755e-01  2.11445763e+00  9.02215459e+01\n",
      "  1.74969649e-02 -1.97850397e-02  1.76067501e+00 -1.31033293e+00\n",
      "  1.88985862e+01  2.85151844e+00]\n",
      "\n",
      "Arrival time: 30.60s\n"
     ]
    }
   ],
   "source": [
    "# Show example of features for one file\n",
    "example_idx = 0\n",
    "print(f'Features for file {files[example_idx]}:')\n",
    "print('Feature vector length:', len(X[example_idx]))\n",
    "print('\\nFirst 10 features:')\n",
    "print(X[example_idx][:10])\n",
    "print(f'\\nArrival time: {y[example_idx]:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417b9d0",
   "metadata": {},
   "source": [
    "## Procesamiento del conjunto de prueba\n",
    "\n",
    "Procesamos el conjunto de prueba usando las mismas funciones de extracción de características que usamos para el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0641d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando tiempos de llegada del conjunto de prueba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29978/1829780971.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_times_df = pd.concat([test_times_df, pd.DataFrame([{\n",
      "100%|██████████| 2500/2500 [00:21<00:00, 115.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempos de llegada guardados en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/val/val_arrival_times.csv\n",
      "y /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/test_arrival_times.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>arrival_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04010919.mseed</td>\n",
       "      <td>14.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04020130.mseed</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04021826.mseed</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04031203.mseed</td>\n",
       "      <td>30.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04040354.mseed</td>\n",
       "      <td>30.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file  arrival_time\n",
       "0  04010919.mseed         14.84\n",
       "1  04020130.mseed         11.45\n",
       "2  04021826.mseed         30.93\n",
       "3  04031203.mseed         30.62\n",
       "4  04040354.mseed         30.26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir rutas\n",
    "testing_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/testing'\n",
    "test_csv_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/raw/VT_P_training.csv'\n",
    "\n",
    "def process_arrival_times(data_path, data_name):\n",
    "    \"\"\"Procesa los tiempos de llegada para el conjunto de prueba.\"\"\"\n",
    "    # Leer CSV con tiempos de llegada de prueba\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Preparar DataFrame para almacenar tiempos de llegada\n",
    "    test_times_df = pd.DataFrame(columns=['file', 'arrival_time'])\n",
    "    \n",
    "    print('Procesando tiempos de llegada del conjunto de prueba...')\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        file_id = row['archivo']\n",
    "        mseed_file = f\"{file_id:08d}.mseed\"\n",
    "        file_path = os.path.join(data_path, mseed_file)\n",
    "        \n",
    "        try:\n",
    "            # Leer señal y obtener tiempo relativo\n",
    "            st = read(file_path)\n",
    "            absolute_p_time = row['lec_p']\n",
    "            relative_p_time = absolute_p_time - st[0].stats.starttime.timestamp\n",
    "            \n",
    "            # Agregar al DataFrame\n",
    "            test_times_df = pd.concat([test_times_df, pd.DataFrame([{\n",
    "                'file': mseed_file,\n",
    "                'arrival_time': relative_p_time\n",
    "            }])], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Guardar tiempos de llegada\n",
    "    test_times_df.to_csv(os.path.join(features_path, f'{data_name}.csv'), index=False)\n",
    "    np.save(os.path.join(features_path, data_name), test_times_df['arrival_time'].values)\n",
    "    \n",
    "    print(f'Tiempos de llegada guardados en {data_path}/{data_name}.csv')\n",
    "    print(f'y {features_path}/test_arrival_times.npy')\n",
    "    \n",
    "    return test_times_df\n",
    "\n",
    "# Procesar tiempos de llegada\n",
    "test_times_df = process_arrival_times(testing_data_path, 'test_arrival_times')\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "#print('\\nPrimeros 5 tiempos de llegada:')\n",
    "# print(test_times_df.head())\n",
    "\n",
    "\n",
    "val_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/val'\n",
    "val_times_df = process_arrival_times(val_data_path, 'val_arrival_times')\n",
    "val_times_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c51bc9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de archivos guardados en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/testing_file_names.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardar los nombres de los archivos en un csv de un directorio \n",
    "def save_file_names(data_path, output_csv):\n",
    "    \"\"\"Guarda los nombres de los archivos en un CSV.\"\"\"\n",
    "    file_names = []\n",
    "    \n",
    "    for root, _, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mseed'):\n",
    "                file_names.append(file)\n",
    "    \n",
    "    # Crear DataFrame y guardar\n",
    "    df = pd.DataFrame(file_names, columns=['file_name'])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f'Nombres de archivos guardados en {output_csv}')\n",
    "\n",
    "# Guardar nombres de archivos de prueba\n",
    "testing_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/testing'\n",
    "\n",
    "output_csv = os.path.join(features_path, 'testing_file_names.csv')\n",
    "save_file_names(testing_data_path, output_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4690b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de archivos guardados en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/val_file_names.csv\n"
     ]
    }
   ],
   "source": [
    "val_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/val'\n",
    "save_file_names(val_data_path, os.path.join(features_path, 'val_file_names.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59bb570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de archivos guardados en /mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/features/train_file_names.csv\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/mnt/c/Users/Usuario/Documents/Studies/GicoProject/SeismicWaves/data/procesed/used_data/training_augmented/train'\n",
    "save_file_names(train_data_path, os.path.join(features_path, 'train_file_names.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b2e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 317 entries, 0 to 316\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   file          317 non-null    object \n",
      " 1   arrival_time  317 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 5.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 496 entries, 0 to 495\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   file          496 non-null    object \n",
      " 1   arrival_time  496 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "val_times_df.info()\n",
    "test_times_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavePredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
